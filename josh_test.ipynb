{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your credentials here\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dataset\n",
    "import json\n",
    "import sys\n",
    "import tweepy\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "db = dataset.connect(os.getenv(\"DATABASE_URL\"))\n",
    "\n",
    "# Variables that contains the credentials to access Twitter API\n",
    "CONSUMER_KEY = os.getenv('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.getenv('CONSUMER_SECRET')\n",
    "ACCESS_KEY = os.getenv('ACCESS_KEY')\n",
    "ACCESS_SECRET = os.getenv('ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "\n",
    "# initialize Tweepy API\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "          wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, output_file=sys.stdout):\n",
    "        super(StreamListener,self).__init__()\n",
    "        self.output_file = output_file\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.counter = self.counter + 1\n",
    "        conditions = (not 'RT @' in status.text)\n",
    "        if conditions:\n",
    "            description = status.user.description\n",
    "            loc = status.user.location\n",
    "            text = status.text\n",
    "            coords = status.coordinates\n",
    "            geo = status.geo\n",
    "            name = status.user.screen_name\n",
    "            user_created = status.user.created_at\n",
    "            id_str = status.id_str\n",
    "            created = status.created_at\n",
    "            source = status.user.url\n",
    "            language = status.lang\n",
    "\n",
    "            if geo is not None:\n",
    "                geo = json.dumps(geo)\n",
    "\n",
    "            if coords is not None:\n",
    "                coords = json.dumps(coords)\n",
    "\n",
    "            table = db[\"tweets\"]\n",
    "            try:\n",
    "                table.insert(dict(\n",
    "                    user_description=description,\n",
    "                    user_location=loc,\n",
    "                    coordinates=coords,\n",
    "                    text=text,\n",
    "                    geo=geo,\n",
    "                    user_name=name,\n",
    "                    user_created=user_created,\n",
    "                    id_str=id_str,\n",
    "                    created=created,\n",
    "                    source = source,\n",
    "                    language = language,\n",
    "                    ))\n",
    "            except ProgrammingError as err:\n",
    "                print(err)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        if status_code == 420:\n",
    "            #return False in on_data disconnects the stream\n",
    "            return False\n",
    "\n",
    "    # When a deleted tweet appears\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "    # When reach the rate limit\n",
    "    def on_limit(self, track):\n",
    "        print(\"Rate limited, continuing\")\n",
    "        # Continue mining tweets\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of the tweepy tweet stream listener\n",
    "stream_listener = StreamListener()\n",
    "\n",
    "# create instance of the tweepy stream\n",
    "stream = tweepy.Stream(auth=auth, listener=stream_listener, tweet_mode=\"extended\")\n",
    "\n",
    "# words to search for\n",
    "track = [\"police\", \"cop\", \"officer\"]\n",
    "\n",
    "# search twitter for programming languages\n",
    "stream.filter(track=track, languages = ['en', 'und'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ids</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1371142519459876865</td>\n      <td>@onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1371142519828926469</td>\n      <td>@SkyNews @skymarkwhite May as well protest whi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1371142519883493379</td>\n      <td>@JMPSimor The right to free assembly and prote...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1371142520684576769</td>\n      <td>@yampylad @therealmissjo The city has accepted...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371142522848874498</td>\n      <td>@CathCarterMusic Absolute disgrace the police ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5598</th>\n      <td>1371252602713870339</td>\n      <td>@richardm680923 @ThisisDavina @sodarkmark @Loo...</td>\n    </tr>\n    <tr>\n      <th>5599</th>\n      <td>1371252604806696960</td>\n      <td>Sachin Vaze|Antilia|Mukesh Ambani |@rautsanjay...</td>\n    </tr>\n    <tr>\n      <th>5600</th>\n      <td>1371252607713427456</td>\n      <td>@OANN Biden,Harris,Pelosi just broke the syste...</td>\n    </tr>\n    <tr>\n      <th>5601</th>\n      <td>1371252609508589570</td>\n      <td>I don't want to put pressure on Patsy, because...</td>\n    </tr>\n    <tr>\n      <th>5602</th>\n      <td>1371252609701531648</td>\n      <td>@MichaelCrane7 @awilliamscomedy Officer, check...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5603 rows × 2 columns</p>\n</div>",
      "text/plain": "                      ids                                               text\n0     1371142519459876865  @onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...\n1     1371142519828926469  @SkyNews @skymarkwhite May as well protest whi...\n2     1371142519883493379  @JMPSimor The right to free assembly and prote...\n3     1371142520684576769  @yampylad @therealmissjo The city has accepted...\n4     1371142522848874498  @CathCarterMusic Absolute disgrace the police ...\n...                   ...                                                ...\n5598  1371252602713870339  @richardm680923 @ThisisDavina @sodarkmark @Loo...\n5599  1371252604806696960  Sachin Vaze|Antilia|Mukesh Ambani |@rautsanjay...\n5600  1371252607713427456  @OANN Biden,Harris,Pelosi just broke the syste...\n5601  1371252609508589570  I don't want to put pressure on Patsy, because...\n5602  1371252609701531648  @MichaelCrane7 @awilliamscomedy Officer, check...\n\n[5603 rows x 2 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.DataFrame(db['tweets'])\n",
    "df = df_raw[[\"id_str\", \"text\"]]\n",
    "df.rename(columns={'id_str': 'ids'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies \n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5603, 2)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NAs and get shape\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ids</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1371142519459876865</td>\n      <td>@onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1371142519828926469</td>\n      <td>@SkyNews @skymarkwhite May as well protest whi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1371142519883493379</td>\n      <td>@JMPSimor The right to free assembly and prote...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1371142520684576769</td>\n      <td>@yampylad @therealmissjo The city has accepted...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371142522848874498</td>\n      <td>@CathCarterMusic Absolute disgrace the police ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   ids                                               text\n0  1371142519459876865  @onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...\n1  1371142519828926469  @SkyNews @skymarkwhite May as well protest whi...\n2  1371142519883493379  @JMPSimor The right to free assembly and prote...\n3  1371142520684576769  @yampylad @therealmissjo The city has accepted...\n4  1371142522848874498  @CathCarterMusic Absolute disgrace the police ..."
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing how the data looks\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"@onlygeek @chrissieA2 @Mike_Fabricant No, I'm not short of criticisms of the police. In fact, I have many - and I d… https://t.co/Jxxe31eKnk\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of text \n",
    "sample = df['text'][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading small version of english nlp\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english parser from spacy\n",
    "parser = English()\n",
    "\n",
    "# boiler-plate tokenize function\n",
    "def tokenize(text):\n",
    "    \"\"\"Parses a string into a list of semantic units (words)\n",
    "    Args: text (str): The string that the function will tokenize.\n",
    "    Returns: list: tokens parsed out by the mechanics of your choice\n",
    "    \"\"\"\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.pos_ == 'PROPN':\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample through function to test outcome\n",
    "tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra stop words that pertains to this model\n",
    "more_stop = ['police', 'officer', 'cop', 'SCREEN_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    \"\"\" takes text and tokenizes it, only looks at tweets with more than 4 words and removes stopwords\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in more_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates column in DF with lemmas\n",
    "df['lemmas'] = df['text'].apply(prepare_text_for_lda)\n",
    "\n",
    "# visualize your work\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python39164bit90a65254d6da4f0d8e4e5fb2b89137a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}