{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your credentials here\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dataset\n",
    "import json\n",
    "import sys\n",
    "import tweepy\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies \n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "db = dataset.connect(os.getenv(\"DATABASE_URL\"))\n",
    "\n",
    "# Variables that contains the credentials to access Twitter API\n",
    "CONSUMER_KEY = os.getenv('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.getenv('CONSUMER_SECRET')\n",
    "ACCESS_KEY = os.getenv('ACCESS_KEY')\n",
    "ACCESS_SECRET = os.getenv('ACCESS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "\n",
    "# initialize Tweepy API\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "          wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamListener(tweepy.StreamListener):\n",
    "    def __init__(self, output_file=sys.stdout):\n",
    "        super(StreamListener,self).__init__()\n",
    "        self.output_file = output_file\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.counter = self.counter + 1\n",
    "        conditions = (not 'RT @' in status.text)\n",
    "        if conditions:\n",
    "            description = status.user.description\n",
    "            loc = status.user.location\n",
    "            text = status.text\n",
    "            coords = status.coordinates\n",
    "            geo = status.geo\n",
    "            name = status.user.screen_name\n",
    "            user_created = status.user.created_at\n",
    "            id_str = status.id_str\n",
    "            created = status.created_at\n",
    "            source = status.user.url\n",
    "            language = status.lang\n",
    "\n",
    "            if geo is not None:\n",
    "                geo = json.dumps(geo)\n",
    "\n",
    "            if coords is not None:\n",
    "                coords = json.dumps(coords)\n",
    "\n",
    "            table = db[\"tweets\"]\n",
    "            try:\n",
    "                table.insert(dict(\n",
    "                    user_description=description,\n",
    "                    user_location=loc,\n",
    "                    coordinates=coords,\n",
    "                    text=text,\n",
    "                    geo=geo,\n",
    "                    user_name=name,\n",
    "                    user_created=user_created,\n",
    "                    id_str=id_str,\n",
    "                    created=created,\n",
    "                    source = source,\n",
    "                    language = language,\n",
    "                    ))\n",
    "            except ProgrammingError as err:\n",
    "                print(err)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        if status_code == 420:\n",
    "            #return False in on_data disconnects the stream\n",
    "            return False\n",
    "\n",
    "    # When a deleted tweet appears\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "    # When reach the rate limit\n",
    "    def on_limit(self, track):\n",
    "        print(\"Rate limited, continuing\")\n",
    "        # Continue mining tweets\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of the tweepy tweet stream listener\n",
    "stream_listener = StreamListener()\n",
    "\n",
    "# create instance of the tweepy stream\n",
    "stream = tweepy.Stream(auth=auth, listener=stream_listener, tweet_mode=\"extended\")\n",
    "\n",
    "# words to search for\n",
    "track = [\"police\", \"cop\", \"officer\"]\n",
    "\n",
    "# search twitter for programming languages\n",
    "stream.filter(track=track, languages = ['en', 'und'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ids</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1371142519459876865</td>\n      <td>@onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1371142519828926469</td>\n      <td>@SkyNews @skymarkwhite May as well protest whi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1371142519883493379</td>\n      <td>@JMPSimor The right to free assembly and prote...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1371142520684576769</td>\n      <td>@yampylad @therealmissjo The city has accepted...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371142522848874498</td>\n      <td>@CathCarterMusic Absolute disgrace the police ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6403</th>\n      <td>1371991058448445440</td>\n      <td>Oh my. ðŸ¥º #StopAsianHate</td>\n    </tr>\n    <tr>\n      <th>6404</th>\n      <td>1371991062550548488</td>\n      <td>It's the @NHL...they only punish based on the ...</td>\n    </tr>\n    <tr>\n      <th>6405</th>\n      <td>1371991068980310018</td>\n      <td>@theonyxshade if i can trick them someones goi...</td>\n    </tr>\n    <tr>\n      <th>6406</th>\n      <td>1371991070813261830</td>\n      <td>16.3.2021 Police setting fires so close to res...</td>\n    </tr>\n    <tr>\n      <th>6407</th>\n      <td>1371991072180740097</td>\n      <td>What do expect in a land where hardly 5-10 % o...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6408 rows Ã— 2 columns</p>\n</div>",
      "text/plain": "                      ids                                               text\n0     1371142519459876865  @onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...\n1     1371142519828926469  @SkyNews @skymarkwhite May as well protest whi...\n2     1371142519883493379  @JMPSimor The right to free assembly and prote...\n3     1371142520684576769  @yampylad @therealmissjo The city has accepted...\n4     1371142522848874498  @CathCarterMusic Absolute disgrace the police ...\n...                   ...                                                ...\n6403  1371991058448445440                            Oh my. ðŸ¥º #StopAsianHate\n6404  1371991062550548488  It's the @NHL...they only punish based on the ...\n6405  1371991068980310018  @theonyxshade if i can trick them someones goi...\n6406  1371991070813261830  16.3.2021 Police setting fires so close to res...\n6407  1371991072180740097  What do expect in a land where hardly 5-10 % o...\n\n[6408 rows x 2 columns]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.DataFrame(db['tweets'])\n",
    "df = df_raw[[\"id_str\", \"text\"]]\n",
    "df.rename(columns={'id_str': 'ids'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = sqlite3.connect('tweets.db')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM tweets\", cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6408, 2)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NAs and get shape\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ids</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1371142519459876865</td>\n      <td>@onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1371142519828926469</td>\n      <td>@SkyNews @skymarkwhite May as well protest whi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1371142519883493379</td>\n      <td>@JMPSimor The right to free assembly and prote...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1371142520684576769</td>\n      <td>@yampylad @therealmissjo The city has accepted...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1371142522848874498</td>\n      <td>@CathCarterMusic Absolute disgrace the police ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                   ids                                               text\n0  1371142519459876865  @onlygeek @chrissieA2 @Mike_Fabricant No, I'm ...\n1  1371142519828926469  @SkyNews @skymarkwhite May as well protest whi...\n2  1371142519883493379  @JMPSimor The right to free assembly and prote...\n3  1371142520684576769  @yampylad @therealmissjo The city has accepted...\n4  1371142522848874498  @CathCarterMusic Absolute disgrace the police ..."
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing how the data looks\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"@onlygeek @chrissieA2 @Mike_Fabricant No, I'm not short of criticisms of the police. In fact, I have many - and I dâ€¦ https://t.co/Jxxe31eKnk\""
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of text \n",
    "sample = df['text'][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fdd0a681311a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generic_type: cannot initialize type \"TensorProtoDataType\": an object with that name is already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bfc59eef30ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BERT imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generic_type: cannot initialize type \"TensorProtoDataType\": an object with that name is already defined"
     ]
    }
   ],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading small version of english nlp\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english parser from spacy\n",
    "parser = English()\n",
    "\n",
    "# boiler-plate tokenize function\n",
    "def tokenize(text):\n",
    "    \"\"\"Parses a string into a list of semantic units (words)\n",
    "    Args: text (str): The string that the function will tokenize.\n",
    "    Returns: list: tokens parsed out by the mechanics of your choice\n",
    "    \"\"\"\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.pos_ == 'PROPN':\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample through function to test outcome\n",
    "tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra stop words that pertains to this model\n",
    "more_stop = ['police', 'officer', 'cop', 'SCREEN_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    \"\"\" takes text and tokenizes it, only looks at tweets with more than 4 words and removes stopwords\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in more_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates column in DF with lemmas\n",
    "df['lemmas'] = df['text'].apply(prepare_text_for_lda)\n",
    "\n",
    "# visualize your work\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}