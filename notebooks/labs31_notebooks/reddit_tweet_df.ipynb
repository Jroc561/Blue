{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_API_KEY = 'XXXXXXXXXXX'\n",
    "TWITTER_API_KEY_SECRET = 'XXXXXXXXXXXXX'\n",
    "TWITTER_AUTH = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_KEY_SECRET)\n",
    "TWITTER = tweepy.API(TWITTER_AUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data currently being used on the website\n",
    "x = requests.get('http://human-rights-first-labs28-c.eba-sfzun5yy.us-east-1.elasticbeanstalk.com/getdata')\n",
    "\n",
    "df = pd.DataFrame(x.json())\n",
    "\n",
    "links = df['links'].to_list()\n",
    "\n",
    "link_lst = [item for sublist in links for item in sublist]\n",
    "twit_lst = [item for item in link_lst if 'twitter' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = []\n",
    "for url in twit_lst:\n",
    "    for item in url.split('/'):\n",
    "        if len(item) == 19 and item[0] == '1':\n",
    "            tweet_ids.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = {'ids':[], 'text':[]}\n",
    "tweets_len = len(tweet_ids)\n",
    "for i,_id in enumerate(tweet_ids):\n",
    "    print(f\"{i+1}/{tweets_len}\")\n",
    "    try:\n",
    "        status = TWITTER.get_status(_id)\n",
    "        tweets['ids'].append(_id)\n",
    "        tweets['text'].append(status.text)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_url(text):\n",
    "    lst = text.split()\n",
    "    lst1 = [word for word in lst if not \"https\" in word]\n",
    "    return \" \".join(lst1)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets_df[\"text1\"] = reddit_tweets_df[\"text\"].apply(strip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets_df.drop(columns=\"text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets_df.rename(columns={'text1':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets_df['reddit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created CSV\n",
    "\n",
    "reddit_tweets_df.to_csv(\"XXXXXXX\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets = pd.read_csv(\"XXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}